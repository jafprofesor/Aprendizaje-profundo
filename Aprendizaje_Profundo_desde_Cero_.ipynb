{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWfHlr15F5hM5fy4dhc5U5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jafprofesor/Aprendizaje-profundo/blob/main/Aprendizaje_Profundo_desde_Cero_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Configuración Inicial**\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "%matplotlib inline\n",
        "print(\"✅ Librerías cargadas!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aKcE6xL5kmlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 1: Perceptrón y Compuerta Lógica**  "
      ],
      "metadata": {
        "id": "D7GjbdXHmAkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **Perceptrón**: Unidad básica de una red neuronal. Toma entradas $x_i$, aplica pesos $w_i$, suma un sesgo $b$, y pasa el resultado por una función de activación.  \n",
        "- **Función Escalón**: Activación binaria (0 o 1) para decisiones simples.  \n",
        "- **Aprendizaje**: Ajuste de pesos mediante `error = etiqueta_real - predicción`.\n",
        "\n",
        "**Implementación:**"
      ],
      "metadata": {
        "id": "50SUVkBckpU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, n_inputs, lr=0.1):\n",
        "        self.weights = np.zeros(n_inputs)  # Inicializar pesos en 0\n",
        "        self.bias = 0                      # Inicializar sesgo en 0\n",
        "        self.lr = lr                       # Tasa de aprendizaje\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"Predicción: inputs · weights + bias\"\"\"\n",
        "        z = np.dot(inputs, self.weights) + self.bias\n",
        "        return 1 if z >= 0 else 0  # Función escalón\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        \"\"\"Entrenamiento: Ajustar pesos con el error\"\"\"\n",
        "        for _ in range(epochs):\n",
        "            for inputs, label in zip(X, y):\n",
        "                y_pred = self.predict(inputs)\n",
        "                error = label - y_pred  # Cálculo del error\n",
        "                # Actualizar pesos y sesgo\n",
        "                self.weights += self.lr * error * inputs\n",
        "                self.bias += self.lr * error\n",
        "\n",
        "# Datos: Compuerta AND\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Entrenar perceptrón\n",
        "p = Perceptron(n_inputs=2, lr=0.1)\n",
        "p.train(X, y, epochs=10)\n",
        "\n",
        "# Probar\n",
        "print(p.predict([1, 1]))  # Debe devolver 1\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Vjysvkjkm_fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 2: Propagación hacia Adelante y Funciones de Activación**  # Nueva sección"
      ],
      "metadata": {
        "id": "QAzBuSoPnV8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **Propagación hacia Adelante**: Cálculo de salidas capa por capa usando $a^{(l)} = f(W^{(l)} \\cdot a^{(l-1)} + b^{(l)})$.  \n",
        "- **Funciones de Activación**:  \n",
        "  - **Sigmoid**: $f(z) = \\frac{1}{1 + e^{-z}}$ (para probabilidades).  \n",
        "  - **ReLU**: $f(z) = \\max(0, z)$ (evita desvanecimiento de gradientes).  \n",
        "\n",
        "**Implementación**:\n"
      ],
      "metadata": {
        "id": "l7VARE0rnhkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Función sigmoid: 1 / (1 + e^(-x))\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"Función ReLU: max(0, x)\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def forward_pass(X, weights, biases):\n",
        "    \"\"\"Cálculo de salidas de la red\"\"\"\n",
        "    # Capa oculta: X · W_hidden + b_hidden\n",
        "    hidden_input = np.dot(X, weights['hidden']) + biases['hidden']\n",
        "    hidden_output = relu(hidden_input)  # Activación ReLU\n",
        "\n",
        "    # Capa salida: hidden_output · W_output + b_output\n",
        "    final_input = np.dot(hidden_output, weights['output']) + biases['output']\n",
        "    return sigmoid(final_input)  # Activación sigmoid\n",
        "\n",
        "# Ejemplo: Red con 3 neuronas ocultas\n",
        "np.random.seed(42)\n",
        "weights = {\n",
        "    'hidden': np.random.randn(2, 3),  # 2 entradas, 3 ocultas\n",
        "    'output': np.random.randn(3, 1)   # 3 ocultas, 1 salida\n",
        "}\n",
        "biases = {'hidden': np.zeros(3), 'output': 0}\n",
        "\n",
        "# Ejecutar propagación\n",
        "X_sample = np.array([[0.5, 0.8]])\n",
        "output = forward_pass(X_sample, weights, biases)\n",
        "print(\"Salida de la red:\", output)\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "goUY6Z69nj3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 3: Función de Pérdida y Retropropagación**  "
      ],
      "metadata": {
        "id": "5_gJdxc2nv5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **Error Cuadrático Medio (MSE)**: $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$.  \n",
        "- **Retropropagación**:  \n",
        "  1. Calcular gradiente de la pérdida respecto a salidas.  \n",
        "  2. Propagarlo hacia atrás usando regla de la cadena.  \n",
        "  3. Actualizar pesos con $\\Delta W = -\\alpha \\frac{\\partial \\text{Pérdida}}{\\partial W}$.  \n",
        "\n",
        "**Implementación**:\n"
      ],
      "metadata": {
        "id": "4FsHaVfUn2Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Error cuadrático medio\"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def backprop(X, y_true, weights, biases, lr=0.1):\n",
        "    # Paso 1: Forward pass (guardar valores intermedios)\n",
        "    hidden_input = np.dot(X, weights['hidden']) + biases['hidden']\n",
        "    hidden_output = relu(hidden_input)\n",
        "    output = sigmoid(np.dot(hidden_output, weights['output']) + biases['output'])\n",
        "\n",
        "    # Paso 2: Calcular gradientes\n",
        "    d_output = (output - y_true) * output * (1 - output)  # Derivada de sigmoid\n",
        "    d_hidden = np.dot(d_output, weights['output'].T) * (hidden_output > 0)  # Derivada de ReLU\n",
        "\n",
        "    # Paso 3: Actualizar parámetros\n",
        "    weights['output'] -= lr * np.dot(hidden_output.T, d_output)\n",
        "    biases['output'] -= lr * np.sum(d_output)\n",
        "    weights['hidden'] -= lr * np.dot(X.T, d_hidden)\n",
        "    biases['hidden'] -= lr * np.sum(d_hidden, axis=0)\n",
        "\n",
        "# Ejemplo de uso\n",
        "X_sample = np.array([[0, 0]])\n",
        "y_true = np.array([[0]])\n",
        "backprop(X_sample, y_true, weights, biases, lr=0.1)\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dj7lu1-8n70p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 4: Descenso de Gradiente Estocástico (SGD)**  "
      ],
      "metadata": {
        "id": "6bs4EQDioIq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **SGD**: Actualiza pesos con mini-lotes de datos en lugar de todo el conjunto.  \n",
        "- **Ventajas**: Más rápido y evita mínimos locales.  \n",
        "\n",
        "**Implementación**:\n"
      ],
      "metadata": {
        "id": "yJZXQtRgoMKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(X, y, weights, biases, lr=0.01, batch_size=32, epochs=100):\n",
        "    n_samples = len(X)\n",
        "    for epoch in range(epochs):\n",
        "        # Mezclar datos en cada época\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_idx = indices[i:i+batch_size]\n",
        "            X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "            backprop(X_batch, y_batch, weights, biases, lr)\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "W4VVf2rFoP5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización del Descenso de Gradiente  "
      ],
      "metadata": {
        "id": "a2zh25mKpCwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear una representación gráfica que muestre cómo el algoritmo de descenso de gradiente encuentra el mínimo de una función de pérdida. Usaremos una función cuadrática simple para ilustrar el proceso.\n",
        "\n",
        "#### **Conceptos Clave**:  \n",
        "- **Función de Pérdida**: Representa el error del modelo (ej. $J(w) = w^2$).  \n",
        "- **Gradiente**: Derivada de la función ($\\nabla J(w) = 2w$).  \n",
        "- **Actualización de Pesos**: $w_{\\text{nuevo}} = w_{\\text{antiguo}} - \\alpha \\nabla J(w)$.  \n",
        "\n",
        "#### **Implementación con Visualización**:\n"
      ],
      "metadata": {
        "id": "vYPZQZLZpLKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Configuración inicial\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "# 1. Definir función de pérdida y su gradiente\n",
        "def loss_function(w):\n",
        "    return w**2  # J(w) = w²\n",
        "\n",
        "def gradient(w):\n",
        "    return 2*w  # ∇J(w) = 2w\n",
        "\n",
        "# 2. Parámetros del descenso de gradiente\n",
        "w_start = 5.0    # Peso inicial\n",
        "learning_rate = 0.2\n",
        "epochs = 15\n",
        "\n",
        "# 3. Almacenar historial de pesos y pérdidas\n",
        "history_w = [w_start]\n",
        "history_loss = [loss_function(w_start)]\n",
        "\n",
        "# 4. Ejecutar descenso de gradiente\n",
        "w = w_start\n",
        "for _ in range(epochs):\n",
        "    grad = gradient(w)  # Calcular gradiente\n",
        "    w = w - learning_rate * grad  # Actualizar peso\n",
        "    history_w.append(w)\n",
        "    history_loss.append(loss_function(w))\n",
        "\n",
        "# 5. Crear gráfico animado\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "w_range = np.linspace(-6, 6, 100)\n",
        "ax.plot(w_range, loss_function(w_range), label='J(w) = w²')\n",
        "ax.set_xlabel('Peso (w)', fontsize=12)\n",
        "ax.set_ylabel('Pérdida', fontsize=12)\n",
        "ax.set_title('Descenso de Gradiente', fontsize=15)\n",
        "point, = ax.plot([], [], 'ro', markersize=10)\n",
        "path, = ax.plot([], [], 'r--', alpha=0.5)\n",
        "\n",
        "def init():\n",
        "    point.set_data([], [])\n",
        "    path.set_data([], [])\n",
        "    return point, path\n",
        "\n",
        "def animate(i):\n",
        "    # Mostrar punto actual y trayectoria\n",
        "    current_w = history_w[i]\n",
        "    current_loss = history_loss[i]\n",
        "    point.set_data([current_w], [current_loss])\n",
        "\n",
        "    # Dibujar trayectoria completa\n",
        "    path.set_data(history_w[:i+1], history_loss[:i+1])\n",
        "\n",
        "    # Añadir flecha de gradiente\n",
        "    if i > 0:\n",
        "        ax.annotate('',\n",
        "                    xy=(history_w[i], history_loss[i]),\n",
        "                    xytext=(history_w[i-1], history_loss[i-1]),\n",
        "                    arrowprops=dict(arrowstyle='->', color='gray', alpha=0.7))\n",
        "\n",
        "    return point, path\n",
        "\n",
        "# 6. Generar animación\n",
        "anim = FuncAnimation(fig, animate, frames=len(history_w),\n",
        "                     init_func=init, blit=True, interval=800)\n",
        "plt.close()\n",
        "\n",
        "# Mostrar en notebook\n",
        "HTML(anim.to_html5_video())\n",
        "```\n"
      ],
      "metadata": {
        "id": "IkjwnnM6pRL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Interpretación del Gráfico**:  \n",
        "1. **Curva Roja**: Función de pérdida $J(w) = w^2$ (con mínimo en $w=0$).  \n",
        "2. **Punto Rojo**: Valor actual del peso ($w$) y su pérdida asociada.  \n",
        "3. **Línea Discontinua**: Trayectoria del descenso de gradiente.  \n",
        "4. **Flechas**: Dirección del movimiento en cada paso.  \n",
        "\n",
        "#### **Efecto de la Tasa de Aprendizaje**:  \n"
      ],
      "metadata": {
        "id": "hxQyRLnTpdxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparar diferentes tasas de aprendizaje\n",
        "learning_rates = [0.01, 0.2, 0.9]\n",
        "colors = ['blue', 'green', 'purple']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "w_range = np.linspace(-6, 6, 100)\n",
        "plt.plot(w_range, loss_function(w_range), 'k-', label='J(w) = w²')\n",
        "\n",
        "for lr, color in zip(learning_rates, colors):\n",
        "    w = 5.0\n",
        "    history_w = [w]\n",
        "    for _ in range(15):\n",
        "        w = w - lr * gradient(w)\n",
        "        history_w.append(w)\n",
        "    plt.plot(history_w, loss_function(np.array(history_w)),\n",
        "             'o--', color=color, alpha=0.7, label=f'LR = {lr}')\n",
        "\n",
        "plt.xlabel('Peso (w)')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.title('Efecto de la Tasa de Aprendizaje')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "```\n"
      ],
      "metadata": {
        "id": "PKbkJ9l7prEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Efecto de la Tasa de Aprendizaje](https://i.imgur.com/8ZQDq7L.png)\n",
        "\n",
        "#### **Conclusiones Visuales**:  \n",
        "- **LR = 0.01**: Convergencia muy lenta (pasos pequeños).  \n",
        "- **LR = 0.2**: Convergencia óptima (llega al mínimo en pocos pasos).  \n",
        "- **LR = 0.9**: Oscila y diverge (pasos demasiado grandes).  \n"
      ],
      "metadata": {
        "id": "MxWcdDawps9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 5: Regularización**  "
      ],
      "metadata": {
        "id": "bu6Bmi-XoZ2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **Overfitting**: Modelo memoriza datos de entrenamiento pero no generaliza.  \n",
        "- **Regularización L2**: Penaliza pesos grandes: $\\text{Pérdida} + \\lambda \\sum w_i^2$.  \n",
        "- **Dropout**: Apaga neuronas aleatoriamente durante entrenamiento.  \n",
        "\n",
        "**Implementación**:\n"
      ],
      "metadata": {
        "id": "PMunhlyDodBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 Regularization\n",
        "def l2_regularization(weights, lambda_val=0.01):\n",
        "    penalty = 0\n",
        "    for key in weights:\n",
        "        penalty += np.sum(weights[key]**2)\n",
        "    return lambda_val * penalty\n",
        "\n",
        "# Dropout en forward pass\n",
        "def forward_dropout(X, weights, dropout_rate=0.5):\n",
        "    mask = (np.random.rand(*X.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    return np.dot(X * mask, weights['hidden'])\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZO4DYffmohvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sección 6: Redes Recurrentes (RNN) y BPTT**  "
      ],
      "metadata": {
        "id": "OTmwV8-XontL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceptos**:  \n",
        "- **RNN**: Neuronas con conexiones recurrentes para datos secuenciales.  \n",
        "- **BPTT**: Backpropagation Through Time: Retropropagación desenrollada en pasos temporales.  \n",
        "\n",
        "**Implementación**:\n"
      ],
      "metadata": {
        "id": "foo_I-jZosYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.Wx = np.random.randn(hidden_size, input_size) * 0.01  # Pesos para entrada\n",
        "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01 # Pesos recurrentes\n",
        "        self.b = np.zeros(hidden_size)                             # Sesgo\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Propagación para una secuencia X\"\"\"\n",
        "        h = [np.zeros(self.Wh.shape[0])]  # Estado oculto inicial\n",
        "        for t in range(len(X)):\n",
        "            h_t = np.tanh(np.dot(self.Wx, X[t]) + np.dot(self.Wh, h[-1]) + self.b)\n",
        "            h.append(h_t)\n",
        "        return h\n",
        "\n",
        "# BPTT: Requiere guardar todos los estados para calcular gradientes temporales\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_LuV2YUJow6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}